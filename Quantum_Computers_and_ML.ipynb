{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGrU0deA3x4T9rHrgmJ0pi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdkdaniel/Quantum-Computers-and-ML-DL/blob/main/Quantum_Computers_and_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A General Review**"
      ],
      "metadata": {
        "id": "2ziBv4LycjYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "Interesting Reads and Resources:\n",
        "\n",
        "\n",
        "*   https://www.nature.com/articles/s41467-021-22539-9\n",
        "*   https://ai.googleblog.com/2021/06/quantum-machine-learning-and-power-of.html\n",
        "*   https://blog.paperspace.com/beginners-guide-to-quantum-machine-learning/\n",
        "*   https://research.ibm.com/topics/quantum-machine-learning\n",
        "*   https://www.nature.com/articles/s41567-021-01287-z\n",
        "\n"
      ],
      "metadata": {
        "id": "g1WyZT22cna9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Good paper on power of data in Quantum ML: https://www.nature.com/articles/s41467-021-22539-9"
      ],
      "metadata": {
        "id": "zEagG2a9q0dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb0OgY-qcc8i"
      },
      "outputs": [],
      "source": [
        "#Quantum advantage over a classical computer is often framed in terms of computational complexity classes. \n",
        "#Examples such as factoring large numbers and simulating quantum systems are classified as bounded quantum polynomial time (BQP) problems.\n",
        "#Which are those thought to be handled more easily by quantum computers than by classical systems. \n",
        "#Problems easily solved on classical computers are called bounded probabilistic polynomial (BPP) problems"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Google Research show:\n",
        "#Learning algorithms equipped with data from a quantum process, such as a natural process like fusion or chemical reactions, form a new class of problems (which they call BPP/Samp).\n",
        "#That efficiently perform some tasks that traditional algorithms without data cannot, and is a subclass of the problems efficiently solvable with polynomial sized advice (P/poly). \n",
        "#This demonstrates that for some machine learning tasks, understanding the quantum advantage requires examination of available data as well."
      ],
      "metadata": {
        "id": "qZm3CIhyoStE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checkout the image showing the intersection of BQP, BPP, BPP/Samp and P/Poly"
      ],
      "metadata": {
        "id": "b3JJAdCZov4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantum Data**"
      ],
      "metadata": {
        "id": "8SAB71BWrDCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the section below, I follow the work of Huang et al. Their work shows how different datasets affect performance comparisons.\n",
        "\n",
        "Furthermore, the authors try to understand how and when classical machine learning models can learn as well as quantum models.\n",
        "\n",
        "To achieve these objectives, the project showcases an empirical performance separation between classical and quantum machine learning model via a carefully crafted dataset.\n",
        "\n",
        "As such, the implementation below:\n",
        "\n",
        "\n",
        "1.   Prepares a reduced dimension Fashion-MNIST dataset.\n",
        "2.   Use quantum circuits to re-label the dataset and compute Projected Quantum Kernel features (PQK).\n",
        "3.   Train a classical neural network on the re-labeled dataset and compare the performance with a model that has access to the PQK features.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rvOSWg2IrGRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dependencies & Setup**"
      ],
      "metadata": {
        "id": "ioTdtpdesw4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.7.0 tensorflow-quantum==0.7.2"
      ],
      "metadata": {
        "id": "wtOYj3Cns4Vm",
        "outputId": "c6308b0b-dc93-447d-cb65-6b6b953a6381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.0%2Bzzzcolab20220506150900-cp37-cp37m-linux_x86_64.whl (665.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 665.5 MB 21 kB/s \n",
            "\u001b[?25hCollecting tensorflow-quantum==0.7.2\n",
            "  Downloading tensorflow_quantum-0.7.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (10.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.4.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.50.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.38.4)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.27.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.14.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.12)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (4.1.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (14.0.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.9.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.3.0)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 52.6 MB/s \n",
            "\u001b[?25hCollecting cirq-core==0.13.1\n",
            "  Downloading cirq_core-0.13.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 47.7 MB/s \n",
            "\u001b[?25hCollecting google-api-core==1.21.0\n",
            "  Downloading google_api_core-1.21.0-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.8 MB/s \n",
            "\u001b[?25hCollecting sympy==1.8\n",
            "  Downloading sympy-1.8-py3-none-any.whl (6.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.1 MB 34.4 MB/s \n",
            "\u001b[?25hCollecting google-auth==1.18.0\n",
            "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 11.6 MB/s \n",
            "\u001b[?25hCollecting googleapis-common-protos==1.52.0\n",
            "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting cirq-google>=0.13.1\n",
            "  Downloading cirq_google-1.0.0-py3-none-any.whl (576 kB)\n",
            "\u001b[K     |████████████████████████████████| 576 kB 64.0 MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.9.2\n",
            "  Downloading protobuf-3.17.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 40.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers~=2.0 in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (2.4.0)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (3.2.2)\n",
            "Collecting duet~=0.2.0\n",
            "  Downloading duet-0.2.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (4.64.1)\n",
            "Requirement already satisfied: networkx~=2.4 in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (2.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (1.3.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core==1.21.0->tensorflow-quantum==0.7.2) (2022.6)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core==1.21.0->tensorflow-quantum==0.7.2) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core==1.21.0->tensorflow-quantum==0.7.2) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.18.0->tensorflow-quantum==0.7.2) (0.2.8)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.18.0->tensorflow-quantum==0.7.2) (4.9)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy==1.8->tensorflow-quantum==0.7.2) (1.2.1)\n",
            "Collecting google-api-core[grpc]<2.0.0dev,>=1.14.0\n",
            "  Downloading google_api_core-1.33.2-py3-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: proto-plus>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from cirq-google>=0.13.1->tensorflow-quantum==0.7.2) (1.22.1)\n",
            "Collecting cirq-google>=0.13.1\n",
            "  Downloading cirq_google-0.15.0-py3-none-any.whl (641 kB)\n",
            "\u001b[K     |████████████████████████████████| 641 kB 64.0 MB/s \n",
            "\u001b[?25h  Downloading cirq_google-0.14.1-py3-none-any.whl (541 kB)\n",
            "\u001b[K     |████████████████████████████████| 541 kB 47.5 MB/s \n",
            "\u001b[?25h  Downloading cirq_google-0.14.0-py3-none-any.whl (541 kB)\n",
            "\u001b[K     |████████████████████████████████| 541 kB 55.3 MB/s \n",
            "\u001b[?25h  Downloading cirq_google-0.13.1-py3-none-any.whl (437 kB)\n",
            "\u001b[K     |████████████████████████████████| 437 kB 70.1 MB/s \n",
            "\u001b[?25hCollecting typing-extensions>=3.6.6\n",
            "  Downloading typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
            "Collecting google-api-core[grpc]<2.0.0dev,>=1.14.0\n",
            "  Downloading google_api_core-1.33.1-py3-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 57.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.33.0-py3-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 58.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.32.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.6-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.5-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.4-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.3-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.8 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.2-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.9 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.1-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.3 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.30.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.29.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.28.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 1.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.27.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.7 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.26.3-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.26.2-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.26.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 1.3 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.26.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 1.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.25.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 306 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.25.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 191 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.24.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 13.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.24.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 13.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.23.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 12.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.22.4-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 12.8 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.22.3-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 8.8 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.22.2-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 11.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.22.1-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 11.7 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.22.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0) (1.5.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (2.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth==1.18.0->tensorflow-quantum==0.7.2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum==0.7.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum==0.7.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum==0.7.2) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum==0.7.2) (2.10)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.2)\n",
            "Installing collected packages: typing-extensions, protobuf, cachetools, googleapis-common-protos, google-auth, sympy, google-api-core, duet, cirq-core, tensorflow-estimator, keras, cirq-google, tensorflow-quantum, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.2.0\n",
            "    Uninstalling cachetools-5.2.0:\n",
            "      Successfully uninstalled cachetools-5.2.0\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.57.0\n",
            "    Uninstalling googleapis-common-protos-1.57.0:\n",
            "      Successfully uninstalled googleapis-common-protos-1.57.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.14.1\n",
            "    Uninstalling google-auth-2.14.1:\n",
            "      Successfully uninstalled google-auth-2.14.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.7.1\n",
            "    Uninstalling sympy-1.7.1:\n",
            "      Successfully uninstalled sympy-1.7.1\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.8.2\n",
            "    Uninstalling google-api-core-2.8.2:\n",
            "      Successfully uninstalled google-api-core-2.8.2\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydata-google-auth 1.4.0 requires google-auth<3.0dev,>=1.25.0; python_version >= \"3.6\", but you have google-auth 1.18.0 which is incompatible.\n",
            "pydantic 1.10.2 requires typing-extensions>=4.1.0, but you have typing-extensions 3.10.0.0 which is incompatible.\n",
            "proto-plus 1.22.1 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 3.17.3 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5, but you have google-api-core 1.21.0 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires google-auth>=1.25.0, but you have google-auth 1.18.0 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-storage 2.5.0 requires google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-storage 2.5.0 requires google-auth<3.0dev,>=1.25.0, but you have google-auth 1.18.0 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-firestore 2.7.2 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-firestore 2.7.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-datastore 2.9.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-datastore 2.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-core 2.3.2 requires google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-core 2.3.2 requires google-auth<3.0dev,>=1.25.0, but you have google-auth 1.18.0 which is incompatible.\n",
            "google-cloud-bigquery 3.3.6 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-bigquery 3.3.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.16.2 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.16.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "firebase-admin 5.3.0 requires google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\", but you have google-api-core 1.21.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cachetools-4.2.4 cirq-core-0.13.1 cirq-google-0.13.1 duet-0.2.7 google-api-core-1.21.0 google-auth-1.18.0 googleapis-common-protos-1.52.0 keras-2.7.0 protobuf-3.17.3 sympy-1.8 tensorflow-2.7.0+zzzcolab20220506150900 tensorflow-estimator-2.7.0 tensorflow-quantum-0.7.2 typing-extensions-3.10.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update package resources to account for version changes.\n",
        "import importlib, pkg_resources\n",
        "importlib.reload(pkg_resources)"
      ],
      "metadata": {
        "id": "CJ6wUcGis5V9",
        "outputId": "b8eee116-1ad1-4a29-f030-d966135a44f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'pkg_resources' from '/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cirq\n",
        "import sympy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "\n",
        "# visualization tools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from cirq.contrib.svg import SVGCircuit\n",
        "np.random.seed(1234)"
      ],
      "metadata": {
        "id": "xcrhpmrSs70z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Prep**"
      ],
      "metadata": {
        "id": "8482lGiJvoL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the fashion-MNIST dataset for running on a quantum computer."
      ],
      "metadata": {
        "id": "YOsu-N6OvwsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Downloading the fashion-MNIST dataset**"
      ],
      "metadata": {
        "id": "f882tzM5v0Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the traditional fashion-mnist dataset. \n",
        "\n",
        "This is done using the tf.keras.datasets module"
      ],
      "metadata": {
        "id": "jUShdLr7v-Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "\n",
        "print(\"Number of original training examples:\", len(x_train))\n",
        "print(\"Number of original test examples:\", len(x_test))\n"
      ],
      "metadata": {
        "id": "YroRiI4ov6K_",
        "outputId": "0a7f1ce0-e213-4cb7-daac-099f2b95dca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "Number of original training examples: 60000\n",
            "Number of original test examples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering the dataset to keep just the T-shirts/tops and dresses.\n",
        "\n",
        "At the same time convert the label, y, to boolean: True for 0 and False for 3."
      ],
      "metadata": {
        "id": "-89caIvwwKPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_03(x, y):\n",
        "    keep = (y == 0) | (y == 3)\n",
        "    x, y = x[keep], y[keep]\n",
        "    y = y == 0\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "F4nPL659vqKB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = filter_03(x_train, y_train)\n",
        "x_test, y_test = filter_03(x_test, y_test)\n",
        "\n",
        "print(\"Number of filtered training examples:\", len(x_train))\n",
        "print(\"Number of filtered test examples:\", len(x_test))"
      ],
      "metadata": {
        "id": "r_RkRekzwT4J",
        "outputId": "8ce0f60a-ca0b-417d-812f-5f713c5f15e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of filtered training examples: 12000\n",
            "Number of filtered test examples: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[0])\n",
        "\n",
        "plt.imshow(x_train[0, :, :])\n",
        "plt.colorbar()\n"
      ],
      "metadata": {
        "id": "To7VQOsVwVeX",
        "outputId": "521a6d7b-7377-4a56-878b-0a1cd738827b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7f8733374d90>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Downscale the images**"
      ],
      "metadata": {
        "id": "5GVxd-8swd3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like the MNIST example, you will need to downscale these images in order to be within the boundaries of current quantum computers. \n",
        "\n",
        "This time however you will use a PCA transformation to reduce the dimensions instead of a tf.image.resize operation."
      ],
      "metadata": {
        "id": "n8DVhTbDwfYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_x(x_train, x_test, n_components=10):\n",
        "  \"\"\"Perform PCA on image dataset keeping the top `n_components` components.\"\"\"\n",
        "  n_points_train = tf.gather(tf.shape(x_train), 0)\n",
        "  n_points_test = tf.gather(tf.shape(x_test), 0)\n",
        "\n",
        "  # Flatten to 1D\n",
        "  x_train = tf.reshape(x_train, [n_points_train, -1])\n",
        "  x_test = tf.reshape(x_test, [n_points_test, -1])\n",
        "\n",
        "  # Normalize.\n",
        "  feature_mean = tf.reduce_mean(x_train, axis=0)\n",
        "  x_train_normalized = x_train - feature_mean\n",
        "  x_test_normalized = x_test - feature_mean\n",
        "\n",
        "  # Truncate.\n",
        "  e_values, e_vectors = tf.linalg.eigh(\n",
        "      tf.einsum('ji,jk->ik', x_train_normalized, x_train_normalized))\n",
        "  return tf.einsum('ij,jk->ik', x_train_normalized, e_vectors[:,-n_components:]), \\\n",
        "    tf.einsum('ij,jk->ik', x_test_normalized, e_vectors[:, -n_components:])\n"
      ],
      "metadata": {
        "id": "9mHlEleSwnv3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIM = 10\n",
        "x_train, x_test = truncate_x(x_train, x_test, n_components=DATASET_DIM)\n",
        "print(f'New datapoint dimension:', len(x_train[0]))\n"
      ],
      "metadata": {
        "id": "BF-D8TdQwqxi",
        "outputId": "f3a13d93-8243-4ffb-86f6-1afe1b79ef6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New datapoint dimension: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step is to reduce the size of the dataset to just 1000 training datapoints and 200 testing datapoints."
      ],
      "metadata": {
        "id": "NWxREeCJwyCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_TRAIN = 1000\n",
        "N_TEST = 200\n",
        "x_train, x_test = x_train[:N_TRAIN], x_test[:N_TEST]\n",
        "y_train, y_test = y_train[:N_TRAIN], y_test[:N_TEST]\n"
      ],
      "metadata": {
        "id": "KXi4IvN-wvue"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"New number of training examples:\", len(x_train))\n",
        "print(\"New number of test examples:\", len(x_test))\n"
      ],
      "metadata": {
        "id": "sMPxXksBw38u",
        "outputId": "a744a610-18d6-4367-daf2-d7319f728074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New number of training examples: 1000\n",
            "New number of test examples: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Relabeling and computing PQK features**"
      ],
      "metadata": {
        "id": "OIoHr-ilw7c6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will now prepare a \"stilted\" quantum dataset by incorporating quantum components and re-labeling the truncated fashion-MNIST dataset you've created above. \n",
        "\n",
        "In order to get the most seperation between quantum and classical methods, you will first prepare the PQK features and then relabel outputs based on their values. "
      ],
      "metadata": {
        "id": "yJd6Bqi7xCba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quantum encoding and PQK features**"
      ],
      "metadata": {
        "id": "S1CyAnqFxWdI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qoHxE4WIxaVN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}